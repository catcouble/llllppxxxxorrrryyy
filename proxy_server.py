import asyncio
import json
import logging
import uuid
import re
import time
from contextlib import asynccontextmanager
from typing import Dict, Optional, Set
from dataclasses import dataclass, field, asdict
from enum import Enum
import signal
import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from starlette.responses import StreamingResponse
import typing
import os
import traceback
from datetime import datetime, timedelta
from collections import defaultdict, deque
import threading
from pathlib import Path
import aiohttp  # å¦‚æœç”¨äºå‘é€HTTPå‘Šè­¦ï¼Œéœ€è¦å…ˆ pip install aiohttp
import gzip
import shutil
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST

# --- Configuration ---
# é…ç½®é¡¹é›†ä¸­ç®¡ç†
class Config:
    # æ—¥å¿—é…ç½®
    LOG_DIR = Path("logs")
    REQUEST_LOG_FILE = "requests.jsonl"
    ERROR_LOG_FILE = "errors.jsonl"
    MAX_LOG_SIZE = 50 * 1024 * 1024  # 50MB
    MAX_LOG_FILES = 50  # ä¿ç•™æœ€å¤š10ä¸ªå†å²æ—¥å¿—æ–‡ä»¶
    
    # æœåŠ¡å™¨é…ç½®
    HOST = "0.0.0.0"
    PORT = 9080
    
    # è¯·æ±‚é…ç½®
    BACKPRESSURE_QUEUE_SIZE = 5
    REQUEST_TIMEOUT_SECONDS = 180
    MAX_CONCURRENT_REQUESTS = 20  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    
    # ç›‘æ§é…ç½®
    STATS_UPDATE_INTERVAL = 5  # ç»Ÿè®¡æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
    CLEANUP_INTERVAL = 300  # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
    
    # å†…å­˜é™åˆ¶
    MAX_ACTIVE_REQUESTS = 100
    MAX_LOG_MEMORY_ITEMS = 1000  # å†…å­˜ä¸­ä¿ç•™çš„æœ€å¤§æ—¥å¿—æ¡ç›®
    MAX_REQUEST_DETAILS = 500  # ä¿ç•™çš„è¯·æ±‚è¯¦æƒ…æ•°é‡

    # ç½‘ç»œé…ç½®
    MANUAL_IP = None  # æ‰‹åŠ¨æŒ‡å®šIPåœ°å€ï¼Œå¦‚ "192.168.0.1"

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
Config.LOG_DIR.mkdir(exist_ok=True)

# --- åŠ¨æ€é…ç½®ç®¡ç† ---
class ConfigManager:
    """ç®¡ç†å¯åŠ¨æ€ä¿®æ”¹çš„é…ç½®"""

    def __init__(self):
        self.config_file = Config.LOG_DIR / "config.json"
        self.dynamic_config = {
            "network": {
                "manual_ip": Config.MANUAL_IP,
                "port": Config.PORT,
                "auto_detect_ip": True
            },
            "request": {
                "timeout_seconds": Config.REQUEST_TIMEOUT_SECONDS,
                "max_concurrent_requests": Config.MAX_CONCURRENT_REQUESTS,
                "backpressure_queue_size": Config.BACKPRESSURE_QUEUE_SIZE
            },
            "quick_links": [
                {"name": "ç›‘æ§é¢æ¿", "url": "/monitor", "icon": "ğŸ“Š"},
                {"name": "å¥åº·æ£€æŸ¥", "url": "/health", "icon": "ğŸ¥"}, 
                {"name": "Prometheus", "url": "/metrics", "icon": "ğŸ“ˆ"},
                {"name": "APIæ–‡æ¡£", "url": "/monitor#api-docs", "icon": "ğŸ“š"}
            ]
        }
        self.load_config()

    def load_config(self):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    saved_config = json.load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    self._deep_merge(self.dynamic_config, saved_config)
                    logging.info("å·²åŠ è½½ä¿å­˜çš„é…ç½®")
            except Exception as e:
                logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.dynamic_config, f, ensure_ascii=False, indent=2)
            logging.info("é…ç½®å·²ä¿å­˜")
        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    def _deep_merge(self, target, source):
        """æ·±åº¦åˆå¹¶å­—å…¸"""
        for key, value in source.items():
            if key in target and isinstance(target[key], dict) and isinstance(value, dict):
                self._deep_merge(target[key], value)
            else:
                target[key] = value

    def get(self, path: str, default=None):
        """è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹å·è·¯å¾„å¦‚ 'network.manual_ip'"""
        keys = path.split('.')
        value = self.dynamic_config
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        return value

    def set(self, path: str, value):
        """è®¾ç½®é…ç½®å€¼"""
        keys = path.split('.')
        target = self.dynamic_config
        for key in keys[:-1]:
            if key not in target:
                target[key] = {}
            target = target[key]
        target[keys[-1]] = value
        self.save_config()

    def get_display_ip(self):
        """è·å–æ˜¾ç¤ºç”¨çš„IPåœ°å€"""
        if self.get('network.manual_ip'):
            return self.get('network.manual_ip')
        elif self.get('network.auto_detect_ip', True):
            return get_local_ip()
        else:
            return "localhost"


# åˆ›å»ºå…¨å±€é…ç½®ç®¡ç†å™¨
config_manager = ConfigManager()


def get_local_ip():
    """è·å–æœ¬æœºå±€åŸŸç½‘IPåœ°å€"""
    import socket
    import platform

    # æ£€æŸ¥æ˜¯å¦æœ‰config_managerå¹¶ä¸”æœ‰æ‰‹åŠ¨é…ç½®çš„IP
    if 'config_manager' in globals():
        manual_ip = config_manager.get('network.manual_ip')
        if manual_ip:
            return manual_ip

    # è·å–æ‰€æœ‰å¯èƒ½çš„IPåœ°å€
    ips = []

    try:
        # æ–¹æ³•1ï¼šè·å–æ‰€æœ‰ç½‘ç»œæ¥å£çš„IP
        hostname = socket.gethostname()
        all_ips = socket.gethostbyname_ex(hostname)[2]

        # è¿‡æ»¤å‡ºå±€åŸŸç½‘IPï¼ˆæ’é™¤è™šæ‹Ÿç½‘å¡ï¼‰
        for ip in all_ips:
            # æ’é™¤å›ç¯åœ°å€å’ŒClashè™šæ‹Ÿç½‘å¡åœ°å€
            if not ip.startswith('127.') and not ip.startswith('198.18.'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç§æœ‰IPåœ°å€
                parts = ip.split('.')
                if len(parts) == 4:
                    first_octet = int(parts[0])
                    second_octet = int(parts[1])

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç§æœ‰IPèŒƒå›´
                    # 10.0.0.0 - 10.255.255.255
                    # 172.16.0.0 - 172.31.255.255
                    # 192.168.0.0 - 192.168.255.255
                    if (first_octet == 10 or
                            (first_octet == 172 and 16 <= second_octet <= 31) or
                            (first_octet == 192 and second_octet == 168)):
                        ips.append(ip)

        # å¦‚æœæ‰¾åˆ°äº†å±€åŸŸç½‘IPï¼Œè¿”å›ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€ä¸»è¦çš„ï¼‰
        if ips:
            # ä¼˜å…ˆè¿”å›192.168å¼€å¤´çš„åœ°å€
            for ip in ips:
                if ip.startswith('192.168.'):
                    return ip
            return ips[0]

        # æ–¹æ³•2ï¼šå¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•è¿æ¥å¤–éƒ¨æœåŠ¡å™¨çš„æ–¹æ³•
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("223.5.5.5", 80))  # ä½¿ç”¨é˜¿é‡ŒDNSè€Œä¸æ˜¯Google
        local_ip = s.getsockname()[0]
        s.close()

        # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ˜¯Clashåœ°å€
        if not local_ip.startswith('198.18.'):
            return local_ip

    except Exception as e:
        logging.warning(f"è·å–IPåœ°å€å¤±è´¥: {e}")

    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›localhost
    return "127.0.0.1"

# é…ç½®Pythonæ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s.%(msecs)03d - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    datefmt='%H:%M:%S',
    handlers=[
        logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
        logging.FileHandler(Config.LOG_DIR / "server.log", encoding='utf-8')  # æ–‡ä»¶è¾“å‡º
    ]
)

# --- Prometheus Metrics ---
# è¯·æ±‚è®¡æ•°å™¨
request_count = Counter(
    'lmarena_requests_total', 
    'Total number of requests',
    ['model', 'status', 'type']
)

# è¯·æ±‚æŒç»­æ—¶é—´ç›´æ–¹å›¾
request_duration = Histogram(
    'lmarena_request_duration_seconds',
    'Request duration in seconds',
    ['model', 'type'],
    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0, float("inf"))
)

# æ´»è·ƒè¯·æ±‚æ•°é‡
active_requests_gauge = Gauge(
    'lmarena_active_requests',
    'Number of active requests'
)

# Tokenä½¿ç”¨è®¡æ•°å™¨
token_usage = Counter(
    'lmarena_tokens_total',
    'Total number of tokens used',
    ['model', 'token_type']  # token_type: input/output
)

# WebSocketè¿æ¥çŠ¶æ€
websocket_status = Gauge(
    'lmarena_websocket_connected',
    'WebSocket connection status (1=connected, 0=disconnected)'
)

# é”™è¯¯è®¡æ•°å™¨
error_count = Counter(
    'lmarena_errors_total',
    'Total number of errors',
    ['error_type', 'model']
)

# æ¨¡å‹æ³¨å†Œæ•°é‡
model_registry_gauge = Gauge(
    'lmarena_models_registered',
    'Number of registered models'
)

# --- Request Details Storage ---
@dataclass
class RequestDetails:
    """å­˜å‚¨è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯"""
    request_id: str
    timestamp: float
    model: str
    status: str
    duration: float
    input_tokens: int
    output_tokens: int
    error: Optional[str]
    request_params: dict
    request_messages: list
    response_content: str
    headers: dict
    
class RequestDetailsStorage:
    """ç®¡ç†è¯·æ±‚è¯¦æƒ…çš„å­˜å‚¨"""
    def __init__(self, max_size: int = Config.MAX_REQUEST_DETAILS):
        self.details: Dict[str, RequestDetails] = {}
        self.order: deque = deque(maxlen=max_size)
        self._lock = threading.Lock()
    
    def add(self, details: RequestDetails):
        """æ·»åŠ è¯·æ±‚è¯¦æƒ…"""
        with self._lock:
            if details.request_id in self.details:
                return
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§å®¹é‡ï¼Œåˆ é™¤æœ€æ—§çš„
            if len(self.order) >= self.order.maxlen:
                oldest_id = self.order[0]
                if oldest_id in self.details:
                    del self.details[oldest_id]
            
            self.details[details.request_id] = details
            self.order.append(details.request_id)
    
    def get(self, request_id: str) -> Optional[RequestDetails]:
        """è·å–è¯·æ±‚è¯¦æƒ…"""
        with self._lock:
            return self.details.get(request_id)
    
    def get_recent(self, limit: int = 100) -> list:
        """è·å–æœ€è¿‘çš„è¯·æ±‚è¯¦æƒ…"""
        with self._lock:
            recent_ids = list(self.order)[-limit:]
            return [self.details[id] for id in reversed(recent_ids) if id in self.details]

# åˆ›å»ºè¯·æ±‚è¯¦æƒ…å­˜å‚¨
request_details_storage = RequestDetailsStorage()

# --- æ—¥å¿—ç®¡ç†å™¨ ---
class LogManager:
    """ç®¡ç†JSON Linesæ ¼å¼çš„æ—¥å¿—æ–‡ä»¶"""
    
    def __init__(self):
        self.request_log_path = Config.LOG_DIR / Config.REQUEST_LOG_FILE
        self.error_log_path = Config.LOG_DIR / Config.ERROR_LOG_FILE
        self._lock = threading.Lock()
        self._check_and_rotate()
    
    def _check_and_rotate(self):
        """æ£€æŸ¥å¹¶è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        for log_path in [self.request_log_path, self.error_log_path]:
            if log_path.exists() and log_path.stat().st_size > Config.MAX_LOG_SIZE:
                self._rotate_log(log_path)
    
    def _rotate_log(self, log_path: Path):
        """è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rotated_path = log_path.with_suffix(f".{timestamp}.jsonl")
        
        # ç§»åŠ¨å½“å‰æ—¥å¿—æ–‡ä»¶
        shutil.move(log_path, rotated_path)
        
        # å‹ç¼©æ—§æ—¥å¿—
        with open(rotated_path, 'rb') as f_in:
            with gzip.open(f"{rotated_path}.gz", 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        # åˆ é™¤æœªå‹ç¼©çš„æ–‡ä»¶
        rotated_path.unlink()
        
        # æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
        self._cleanup_old_logs()
    
    def _cleanup_old_logs(self):
        """æ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶"""
        log_files = sorted(Config.LOG_DIR.glob("*.jsonl.gz"), key=lambda x: x.stat().st_mtime)
        
        # ä¿ç•™æœ€æ–°çš„Nä¸ªæ–‡ä»¶
        while len(log_files) > Config.MAX_LOG_FILES:
            oldest_file = log_files.pop(0)
            oldest_file.unlink()
            logging.info(f"åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶: {oldest_file}")
    
    def write_request_log(self, log_entry: dict):
        """å†™å…¥è¯·æ±‚æ—¥å¿—"""
        with self._lock:
            self._check_and_rotate()
            with open(self.request_log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def write_error_log(self, log_entry: dict):
        """å†™å…¥é”™è¯¯æ—¥å¿—"""
        with self._lock:
            self._check_and_rotate()
            with open(self.error_log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def read_request_logs(self, limit: int = 100, offset: int = 0, model: str = None) -> list:
        """è¯»å–è¯·æ±‚æ—¥å¿—"""
        logs = []
        
        # è¯»å–å½“å‰æ—¥å¿—æ–‡ä»¶
        if self.request_log_path.exists():
            with open(self.request_log_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()
                
                # åå‘è¯»å–ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                for line in reversed(all_lines):
                    try:
                        log = json.loads(line.strip())
                        if log.get('type') == 'request_end':  # åªè¿”å›å®Œæˆçš„è¯·æ±‚
                            if model and log.get('model') != model:
                                continue
                            logs.append(log)
                            if len(logs) >= limit + offset:
                                break
                    except json.JSONDecodeError:
                        continue
        
        # è¿”å›æŒ‡å®šèŒƒå›´çš„æ—¥å¿—
        return logs[offset:offset + limit]
    
    def read_error_logs(self, limit: int = 50) -> list:
        """è¯»å–é”™è¯¯æ—¥å¿—"""
        logs = []
        
        if self.error_log_path.exists():
            with open(self.error_log_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()
                
                # åå‘è¯»å–ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                for line in reversed(all_lines[-limit:]):
                    try:
                        log = json.loads(line.strip())
                        logs.append(log)
                    except json.JSONDecodeError:
                        continue
        
        return logs

# åˆ›å»ºå…¨å±€æ—¥å¿—ç®¡ç†å™¨
log_manager = LogManager()

# --- æ€§èƒ½ç›‘æ§ ---
class PerformanceMonitor:
    """ç®€åŒ–çš„æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.request_times = deque(maxlen=1000)
        self.model_stats = defaultdict(lambda: {'count': 0, 'errors': 0})
    
    def record_request(self, model: str, duration: float, success: bool):
        """è®°å½•è¯·æ±‚æ€§èƒ½"""
        self.request_times.append(duration)
        self.model_stats[model]['count'] += 1
        if not success:
            self.model_stats[model]['errors'] += 1
    
    def get_stats(self) -> dict:
        """è·å–åŸºæœ¬ç»Ÿè®¡"""
        if not self.request_times:
            return {'avg_response_time': 0}
        return {
            'avg_response_time': sum(self.request_times) / len(self.request_times)
        }


# åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
performance_monitor = PerformanceMonitor()

# --- WebSocketå¿ƒè·³ç®¡ç† ---
class WebSocketHeartbeat:
    def __init__(self, interval: int = 30):
        self.interval = interval
        self.last_ping = time.time()
        self.last_pong = time.time()
        self.missed_pongs = 0
        self.max_missed_pongs = 3

    async def start_heartbeat(self, ws: WebSocket):
        """å¯åŠ¨å¿ƒè·³ä»»åŠ¡"""
        while not SHUTTING_DOWN and ws:
            try:
                current_time = time.time()

                # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°pongå“åº”
                if current_time - self.last_pong > self.interval * 2:
                    self.missed_pongs += 1
                    if self.missed_pongs >= self.max_missed_pongs:
                        logging.warning("å¿ƒè·³è¶…æ—¶ï¼Œæµè§ˆå™¨å¯èƒ½å·²æ–­çº¿")
                        await self.notify_disconnect()
                        break

                # å‘é€ping
                await ws.send_text(json.dumps({"type": "ping", "timestamp": current_time}))
                self.last_ping = current_time

                await asyncio.sleep(self.interval)

            except Exception as e:
                logging.error(f"å¿ƒè·³å‘é€å¤±è´¥: {e}")
                break

    def handle_pong(self):
        """å¤„ç†pongå“åº”"""
        self.last_pong = time.time()
        self.missed_pongs = 0

    async def notify_disconnect(self):
        """é€šçŸ¥ç›‘æ§é¢æ¿è¿æ¥æ–­å¼€"""
        logging.warning("æµè§ˆå™¨WebSocketå¿ƒè·³è¶…æ—¶")




# --- å®æ—¶ç»Ÿè®¡æ•°æ®ç»“æ„ ---
@dataclass
class RealtimeStats:
    active_requests: Dict[str, dict] = field(default_factory=dict)
    recent_requests: deque = field(default_factory=lambda: deque(maxlen=Config.MAX_LOG_MEMORY_ITEMS))
    recent_errors: deque = field(default_factory=lambda: deque(maxlen=50))
    model_usage: Dict[str, dict] = field(default_factory=lambda: defaultdict(lambda: {
        'requests': 0, 'tokens': 0, 'errors': 0, 'avg_duration': 0
    }))
    
    def cleanup_old_requests(self):
        """æ¸…ç†è¶…æ—¶çš„æ´»è·ƒè¯·æ±‚"""
        current_time = time.time()
        timeout_requests = []
        
        for req_id, req in self.active_requests.items():
            if current_time - req['start_time'] > Config.REQUEST_TIMEOUT_SECONDS:
                timeout_requests.append(req_id)
        
        for req_id in timeout_requests:
            logging.warning(f"æ¸…ç†è¶…æ—¶è¯·æ±‚: {req_id}")
            del self.active_requests[req_id]

realtime_stats = RealtimeStats()

# --- æ¸…ç†ä»»åŠ¡ ---
async def periodic_cleanup():
    """å®šæœŸæ¸…ç†ä»»åŠ¡"""
    while not SHUTTING_DOWN:
        try:
            # æ¸…ç†è¶…æ—¶çš„æ´»è·ƒè¯·æ±‚
            realtime_stats.cleanup_old_requests()
            
            # è§¦å‘æ—¥å¿—è½®è½¬æ£€æŸ¥
            log_manager._check_and_rotate()
            
            # æ›´æ–°PrometheusæŒ‡æ ‡
            active_requests_gauge.set(len(realtime_stats.active_requests))
            model_registry_gauge.set(len(MODEL_REGISTRY))
            
            logging.info(f"æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæˆ. æ´»è·ƒè¯·æ±‚: {len(realtime_stats.active_requests)}")
            
        except Exception as e:
            logging.error(f"æ¸…ç†ä»»åŠ¡å‡ºé”™: {e}")
        
        await asyncio.sleep(Config.CLEANUP_INTERVAL)

# --- Custom Streaming Response with Immediate Flush ---
class ImmediateStreamingResponse(StreamingResponse):
    """Custom streaming response that forces immediate flushing of chunks"""

    async def stream_response(self, send: typing.Callable) -> None:
        await send({
            "type": "http.response.start",
            "status": self.status_code,
            "headers": self.raw_headers,
        })

        async for chunk in self.body_iterator:
            if chunk:
                # Send the chunk immediately
                await send({
                    "type": "http.response.body",
                    "body": chunk.encode(self.charset) if isinstance(chunk, str) else chunk,
                    "more_body": True,
                })
                # Force a small delay to ensure the chunk is sent
                await asyncio.sleep(0)

        # Send final empty chunk to close the stream
        await send({
            "type": "http.response.body",
            "body": b"",
            "more_body": False,
        })


# --- Request State Management ---
class RequestStatus(Enum):
    PENDING = "pending"
    SENT_TO_BROWSER = "sent_to_browser"
    PROCESSING = "processing"
    COMPLETED = "completed"
    TIMEOUT = "timeout"
    ERROR = "error"


@dataclass
class PersistentRequest:
    request_id: str
    openai_request: dict
    response_queue: asyncio.Queue
    status: RequestStatus = RequestStatus.PENDING
    created_at: float = field(default_factory=time.time)
    sent_to_browser_at: Optional[float] = None
    last_activity_at: Optional[float] = None
    model_name: str = ""
    is_streaming: bool = True
    accumulated_response: str = ""  # å­˜å‚¨å“åº”å†…å®¹


class PersistentRequestManager:
    def __init__(self):
        self.active_requests: Dict[str, PersistentRequest] = {}
        self._lock = asyncio.Lock()

    async def add_request(self, request_id: str, openai_request: dict, response_queue: asyncio.Queue,
                    model_name: str, is_streaming: bool) -> PersistentRequest:
        """Add a new request to be tracked"""
        async with self._lock:
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¹¶å‘æ•°
            if len(self.active_requests) >= Config.MAX_CONCURRENT_REQUESTS:
                raise HTTPException(status_code=503, detail="Too many concurrent requests")
            
            persistent_req = PersistentRequest(
                request_id=request_id,
                openai_request=openai_request,
                response_queue=response_queue,
                model_name=model_name,
                is_streaming=is_streaming
            )
            self.active_requests[request_id] = persistent_req
            
            # æ›´æ–°PrometheusæŒ‡æ ‡
            active_requests_gauge.inc()
            
            logging.info(f"REQUEST_MGR: Added request {request_id} for tracking")
            return persistent_req

    def get_request(self, request_id: str) -> Optional[PersistentRequest]:
        """Get a request by ID"""
        return self.active_requests.get(request_id)

    def update_status(self, request_id: str, status: RequestStatus):
        """Update request status"""
        if request_id in self.active_requests:
            self.active_requests[request_id].status = status
            self.active_requests[request_id].last_activity_at = time.time()
            logging.debug(f"REQUEST_MGR: Updated request {request_id} status to {status.value}")

    def mark_sent_to_browser(self, request_id: str):
        """Mark request as sent to browser"""
        if request_id in self.active_requests:
            self.active_requests[request_id].sent_to_browser_at = time.time()
            self.update_status(request_id, RequestStatus.SENT_TO_BROWSER)

    async def timeout_request(self, request_id: str):
        """Timeout a request and send error to client"""
        if request_id in self.active_requests:
            req = self.active_requests[request_id]
            req.status = RequestStatus.TIMEOUT

            # Send timeout error to client
            try:
                await req.response_queue.put({
                    "error": f"Request timed out after {Config.REQUEST_TIMEOUT_SECONDS} seconds. Browser may have disconnected during Cloudflare challenge."
                })
            except KeyboardInterrupt:
                raise
            except Exception as e:
                logging.error(f"REQUEST_MGR: Error sending timeout to queue for {request_id}: {e}")

            # Remove from active requests
            del self.active_requests[request_id]
            active_requests_gauge.dec()
            logging.warning(f"REQUEST_MGR: Request {request_id} timed out and removed")

    def complete_request(self, request_id: str):
        """Mark request as completed and remove from tracking"""
        if request_id in self.active_requests:
            self.active_requests[request_id].status = RequestStatus.COMPLETED
            del self.active_requests[request_id]
            active_requests_gauge.dec()
            logging.info(f"REQUEST_MGR: Request {request_id} completed and removed")

    def get_pending_requests(self) -> Dict[str, PersistentRequest]:
        """Get all requests that were sent to browser but not completed"""
        return {
            req_id: req for req_id, req in self.active_requests.items()
            if req.status in [RequestStatus.SENT_TO_BROWSER, RequestStatus.PROCESSING]
        }

    async def request_timeout_watcher(self, requests_to_watch: Dict[str, PersistentRequest]):
        """A background task to watch for and time out disconnected requests."""
        try:
            await asyncio.sleep(Config.REQUEST_TIMEOUT_SECONDS)

            logging.info(f"WATCHER: Timeout reached. Checking {len(requests_to_watch)} requests.")
            for request_id, req in requests_to_watch.items():
                # Check if the request is still pending (i.e., not completed by a reconnect)
                if self.get_request(request_id) and req.status in [RequestStatus.SENT_TO_BROWSER,
                                                                   RequestStatus.PROCESSING]:
                    logging.warning(f"WATCHER: Request {request_id} timed out after browser disconnect.")
                    await self.timeout_request(request_id)
        except asyncio.CancelledError:
            logging.info("WATCHER: Request timeout watcher was cancelled, likely due to server shutdown.")
        except Exception as e:
            logging.error(f"WATCHER: Error in request timeout watcher: {e}", exc_info=True)

    async def handle_browser_disconnect(self):
        """Handle browser WebSocket disconnect - spawn timeout watchers for pending requests."""
        pending_requests = self.get_pending_requests()
        if not pending_requests:
            return

        logging.warning(f"REQUEST_MGR: Browser disconnected with {len(pending_requests)} pending requests.")

        # Check if we're shutting down
        global SHUTTING_DOWN
        if SHUTTING_DOWN:
            # During shutdown, timeout immediately to avoid hanging
            logging.info("REQUEST_MGR: Server shutting down, timing out all pending requests immediately.")
            for request_id in list(pending_requests.keys()):
                logging.info(f"REQUEST_MGR: Timing out request {request_id} due to shutdown.")
                await self.timeout_request(request_id)
        else:
            # During normal operation, spawn a watcher task for the timeout
            logging.info(f"REQUEST_MGR: Spawning timeout watcher for {len(pending_requests)} pending requests.")
            watcher_task = asyncio.create_task(self.request_timeout_watcher(pending_requests.copy()))
            background_tasks.add(watcher_task)
            watcher_task.add_done_callback(background_tasks.discard)


# --- Logging Functions ---
def log_request_start(request_id: str, model: str, params: dict, messages: list = None):
    """è®°å½•è¯·æ±‚å¼€å§‹"""
    request_info = {
        'id': request_id,
        'model': model,
        'start_time': time.time(),
        'status': 'active',
        'params': params,
        'messages': messages or []
    }
    
    realtime_stats.active_requests[request_id] = request_info
    
    # å†™å…¥æ—¥å¿—æ–‡ä»¶
    log_entry = {
        'type': 'request_start',
        'timestamp': time.time(),
        'request_id': request_id,
        'model': model,
        'params': params
    }
    log_manager.write_request_log(log_entry)
    
def log_request_end(request_id: str, success: bool, input_tokens: int = 0, 
                   output_tokens: int = 0, error: str = None, response_content: str = ""):
    """è®°å½•è¯·æ±‚ç»“æŸ"""
    if request_id not in realtime_stats.active_requests:
        return
        
    req = realtime_stats.active_requests[request_id]
    duration = time.time() - req['start_time']
    
    # æ›´æ–°å®æ—¶ç»Ÿè®¡
    req['status'] = 'success' if success else 'failed'
    req['duration'] = duration
    req['input_tokens'] = input_tokens
    req['output_tokens'] = output_tokens
    req['error'] = error
    req['end_time'] = time.time()
    req['response_content'] = response_content
    
    # æ·»åŠ åˆ°æœ€è¿‘è¯·æ±‚åˆ—è¡¨
    realtime_stats.recent_requests.append(req.copy())
    
    # æ›´æ–°æ¨¡å‹ç»Ÿè®¡
    model = req['model']
    stats = realtime_stats.model_usage[model]
    stats['requests'] += 1
    if success:
        stats['tokens'] += input_tokens + output_tokens
    else:
        stats['errors'] += 1
    
    # è®°å½•æ€§èƒ½
    performance_monitor.record_request(model, duration, success)
    
    # æ›´æ–°PrometheusæŒ‡æ ‡
    request_count.labels(model=model, status='success' if success else 'failed', type='chat').inc()
    request_duration.labels(model=model, type='chat').observe(duration)
    token_usage.labels(model=model, token_type='input').inc(input_tokens)
    token_usage.labels(model=model, token_type='output').inc(output_tokens)
    
    # ä¿å­˜è¯·æ±‚è¯¦æƒ…
    details = RequestDetails(
        request_id=request_id,
        timestamp=req['start_time'],
        model=model,
        status='success' if success else 'failed',
        duration=duration,
        input_tokens=input_tokens,
        output_tokens=output_tokens,
        error=error,
        request_params=req.get('params', {}),
        request_messages=req.get('messages', []),
        response_content=response_content[:5000],  # é™åˆ¶é•¿åº¦
        headers={}
    )
    request_details_storage.add(details)
    
    # å†™å…¥æ—¥å¿—æ–‡ä»¶
    log_entry = {
        'type': 'request_end',
        'timestamp': time.time(),
        'request_id': request_id,
        'model': model,
        'status': 'success' if success else 'failed',
        'duration': duration,
        'input_tokens': input_tokens,
        'output_tokens': output_tokens,
        'error': error,
        'params': req.get('params', {})
    }
    log_manager.write_request_log(log_entry)
    
    # ä»æ´»åŠ¨è¯·æ±‚ä¸­ç§»é™¤
    del realtime_stats.active_requests[request_id]

def log_error(request_id: str, error_type: str, error_message: str, stack_trace: str = ""):
    """è®°å½•é”™è¯¯æ—¥å¿—"""
    error_data = {
        'timestamp': time.time(),
        'request_id': request_id,
        'error_type': error_type,
        'error_message': error_message,
        'stack_trace': stack_trace
    }
    
    realtime_stats.recent_errors.append(error_data)
    
    # æ›´æ–°PrometheusæŒ‡æ ‡
    model = realtime_stats.active_requests.get(request_id, {}).get('model', 'unknown')
    error_count.labels(error_type=error_type, model=model).inc()
    
    # å†™å…¥é”™è¯¯æ—¥å¿—æ–‡ä»¶
    log_manager.write_error_log(error_data)

# --- Model Registry ---
MODEL_REGISTRY = {}  # Will be populated dynamically


def update_model_registry(models_data: dict) -> None:
    """Update the model registry with data from browser, inferring type from capabilities."""
    global MODEL_REGISTRY

    try:
        if not models_data or not isinstance(models_data, dict):
            logging.warning(f"Received empty or invalid model data: {models_data}")
            return

        new_registry = {}
        for public_name, model_info in models_data.items():
            if not isinstance(model_info, dict):
                continue

            # Determine type from outputCapabilities
            model_type = "chat"  # Default
            capabilities = model_info.get("capabilities", {})
            if isinstance(capabilities, dict):
                output_caps = capabilities.get("outputCapabilities", {})
                if isinstance(output_caps, dict):
                    if "image" in output_caps:
                        model_type = "image"
                    elif "video" in output_caps:
                        model_type = "video"

            # Store the processed model info with the determined type
            processed_info = model_info.copy()
            processed_info["type"] = model_type
            new_registry[public_name] = processed_info

        MODEL_REGISTRY = new_registry
        model_registry_gauge.set(len(MODEL_REGISTRY))
        logging.info(f"Updated and processed model registry with {len(MODEL_REGISTRY)} models.")

    except KeyboardInterrupt:
        raise
    except Exception as e:
        logging.error(f"Error updating model registry: {e}", exc_info=True)


def get_fallback_registry():
    """Fallback registry in case dynamic fetching fails."""
    return {}


# --- Global State ---
browser_ws: WebSocket | None = None
response_channels: dict[str, asyncio.Queue] = {}  # Keep for backward compatibility
request_manager = PersistentRequestManager()
background_tasks: Set[asyncio.Task] = set()
SHUTTING_DOWN = False
monitor_clients: Set[WebSocket] = set()  # ç›‘æ§å®¢æˆ·ç«¯è¿æ¥
startup_time = time.time()  # æœåŠ¡å™¨å¯åŠ¨æ—¶é—´

# --- Helper Functions for Monitoring ---
async def broadcast_to_monitors(data: dict):
    """å‘æ‰€æœ‰ç›‘æ§å®¢æˆ·ç«¯å¹¿æ’­æ•°æ®"""
    if not monitor_clients:
        return
        
    disconnected = []
    for client in monitor_clients:
        try:
            await client.send_json(data)
        except:
            disconnected.append(client)
    
    # æ¸…ç†æ–­å¼€çš„è¿æ¥
    for client in disconnected:
        monitor_clients.discard(client)

# --- FastAPI App and Lifespan ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global MODEL_REGISTRY, request_manager, startup_time
    logging.info(f"æœåŠ¡å™¨æ­£åœ¨å¯åŠ¨...")
    startup_time = time.time()

    # æ˜¾ç¤ºè®¿é—®åœ°å€
    local_ip = get_local_ip()
    logging.info(f"ğŸŒ Server access URLs:")
    logging.info(f"  - Local: http://localhost:{Config.PORT}")
    logging.info(f"  - Network: http://{local_ip}:{Config.PORT}")
    logging.info(f"ğŸ“± Use the Network URL to access from your phone on the same WiFi")

    # === æ·»åŠ è¯¦ç»†çš„ç«¯ç‚¹è¯´æ˜ ===
    logging.info(f"\nğŸ“‹ Available Endpoints:")
    logging.info(f"  ğŸ–¥ï¸  Monitor Dashboard: http://{local_ip}:{Config.PORT}/monitor")
    logging.info(f"     å®æ—¶ç›‘æ§é¢æ¿ï¼ŒæŸ¥çœ‹ç³»ç»ŸçŠ¶æ€ã€è¯·æ±‚æ—¥å¿—ã€æ€§èƒ½æŒ‡æ ‡")

    logging.info(f"\n  ğŸ“Š Metrics & Health:")
    logging.info(f"     - Prometheus Metrics: http://{local_ip}:{Config.PORT}/metrics")
    logging.info(f"       Prometheusæ ¼å¼çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¯æ¥å…¥Grafana")
    logging.info(f"     - Health Check: http://{local_ip}:{Config.PORT}/health")
    logging.info(f"       åŸºç¡€å¥åº·æ£€æŸ¥")

    logging.info(f"\n  ğŸ¤– AI API:")
    logging.info(f"     - Chat Completions: POST http://{local_ip}:{Config.PORT}/v1/chat/completions")
    logging.info(f"       OpenAIå…¼å®¹çš„èŠå¤©API")
    logging.info(f"     - List Models: GET http://{local_ip}:{Config.PORT}/v1/models")
    logging.info(f"       è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    logging.info(f"     - Refresh Models: POST http://{local_ip}:{Config.PORT}/v1/refresh-models")
    logging.info(f"       åˆ·æ–°æ¨¡å‹åˆ—è¡¨")

    logging.info(f"\n  ğŸ“ˆ Statistics:")
    logging.info(f"     - Stats Summary: http://{local_ip}:{Config.PORT}/api/stats/summary")
    logging.info(f"       24å°æ—¶ç»Ÿè®¡æ‘˜è¦")
    logging.info(f"     - Request Logs: http://{local_ip}:{Config.PORT}/api/logs/requests")
    logging.info(f"       è¯·æ±‚æ—¥å¿—API")
    logging.info(f"     - Error Logs: http://{local_ip}:{Config.PORT}/api/logs/errors")
    logging.info(f"       é”™è¯¯æ—¥å¿—API")
    logging.info(f"     - Alerts: http://{local_ip}:{Config.PORT}/api/alerts")
    logging.info(f"       ç³»ç»Ÿå‘Šè­¦å†å²")

    logging.info(f"\n  ğŸ› ï¸  OpenAI Client Config:")
    logging.info(f"     base_url='http://{local_ip}:{Config.PORT}/v1'")
    logging.info(f"     api_key='sk-any-string-you-like'")
    logging.info(f"\n{'=' * 60}\n")
    # === ç»“æŸæ·»åŠ  ===

    # Use fallback registry on startup - models will be updated by browser script
    MODEL_REGISTRY = get_fallback_registry()
    logging.info(f"å·²åŠ è½½ {len(MODEL_REGISTRY)} ä¸ªå¤‡ç”¨æ¨¡å‹")

    # å¯åŠ¨æ¸…ç†ä»»åŠ¡
    cleanup_task = asyncio.create_task(periodic_cleanup())
    background_tasks.add(cleanup_task)

    logging.info("æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")

    try:
        yield
    finally:
        global SHUTTING_DOWN
        SHUTTING_DOWN = True
        logging.info(f"ç”Ÿå‘½å‘¨æœŸ: æœåŠ¡å™¨æ­£åœ¨å…³é—­ã€‚æ­£åœ¨å–æ¶ˆ {len(background_tasks)} ä¸ªåå°ä»»åŠ¡...")

        # Cancel all background tasks
        cancelled_tasks = []
        for task in list(background_tasks):
            if not task.done():
                logging.info(f"ç”Ÿå‘½å‘¨æœŸ: æ­£åœ¨å–æ¶ˆä»»åŠ¡: {task}")
                task.cancel()
                cancelled_tasks.append(task)

        # Wait for cancelled tasks to finish
        if cancelled_tasks:
            logging.info(f"ç”Ÿå‘½å‘¨æœŸ: ç­‰å¾… {len(cancelled_tasks)} ä¸ªå·²å–æ¶ˆçš„ä»»åŠ¡å®Œæˆ...")
            results = await asyncio.gather(*cancelled_tasks, return_exceptions=True)
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logging.info(f"ç”Ÿå‘½å‘¨æœŸ: ä»»åŠ¡ {i} å®Œæˆï¼Œç»“æœ: {type(result).__name__}")
                else:
                    logging.info(f"ç”Ÿå‘½å‘¨æœŸ: ä»»åŠ¡ {i} æ­£å¸¸å®Œæˆ")

        logging.info("ç”Ÿå‘½å‘¨æœŸ: æ‰€æœ‰åå°ä»»åŠ¡å·²å–æ¶ˆã€‚å…³é—­å®Œæˆã€‚")


app = FastAPI(lifespan=lifespan)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å¼€å‘ç¯å¢ƒå¯ä»¥ç”¨*ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®æ”¹ä¸ºå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

from fastapi.staticfiles import StaticFiles
app.mount("/static", StaticFiles(directory="static"), name="static")


# --- WebSocket Handler (Producer) ---
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    global browser_ws, request_manager
    await websocket.accept()
    logging.info("âœ… æµè§ˆå™¨WebSocketå·²è¿æ¥")
    browser_ws = websocket
    websocket_status.set(1)

    # åˆ›å»ºå¿ƒè·³å®ä¾‹å¹¶å¯åŠ¨å¿ƒè·³ä»»åŠ¡
    heartbeat = WebSocketHeartbeat()
    heartbeat_task = asyncio.create_task(heartbeat.start_heartbeat(websocket))
    background_tasks.add(heartbeat_task)

    # Handle reconnection - check for pending requests
    pending_requests = request_manager.get_pending_requests()
    if pending_requests:
        logging.info(f"ğŸ”„ æµè§ˆå™¨é‡è¿ï¼Œæœ‰ {len(pending_requests)} ä¸ªå¾…å¤„ç†è¯·æ±‚")

        # Send reconnection acknowledgment with pending request IDs
        await websocket.send_text(json.dumps({
            "type": "reconnection_ack",
            "pending_request_ids": list(pending_requests.keys()),
            "message": f"å·²é‡è¿ã€‚å‘ç° {len(pending_requests)} ä¸ªå¾…å¤„ç†è¯·æ±‚ã€‚"
        }))

    try:
        while True:
            message_str = await websocket.receive_text()
            message = json.loads(message_str)
            # å¤„ç†pongå“åº”
            if message.get("type") == "pong":
                heartbeat.handle_pong()
                continue


            # Handle reconnection handshake from browser
            if message.get("type") == "reconnection_handshake":
                browser_pending_ids = message.get("pending_request_ids", [])
                logging.info(f"ğŸ¤ æ”¶åˆ°é‡è¿æ¡æ‰‹ï¼Œæµè§ˆå™¨æœ‰ {len(browser_pending_ids)} ä¸ªå¾…å¤„ç†è¯·æ±‚")

                # Restore request channels for matching requests
                restored_count = 0
                for request_id in browser_pending_ids:
                    persistent_req = request_manager.get_request(request_id)
                    if persistent_req:
                        # Restore the response channel
                        response_channels[request_id] = persistent_req.response_queue
                        request_manager.update_status(request_id, RequestStatus.PROCESSING)
                        restored_count += 1
                        logging.info(f"ğŸ”„ å·²æ¢å¤è¯·æ±‚é€šé“: {request_id}")

                # Send restoration acknowledgment
                await websocket.send_text(json.dumps({
                    "type": "restoration_ack",
                    "restored_count": restored_count,
                    "message": f"å·²æ¢å¤ {restored_count} ä¸ªè¯·æ±‚é€šé“"
                }))
                continue

            # Handle model registry updates
            if message.get("type") == "model_registry":
                models_data = message.get("models", {})
                update_model_registry(models_data)

                # Send acknowledgment
                await websocket.send_text(json.dumps({
                    "type": "model_registry_ack",
                    "count": len(MODEL_REGISTRY)
                }))
                continue

            # Handle regular chat requests
            request_id = message.get("request_id")
            data = message.get("data")
            logging.debug(f"â¬…ï¸ æµè§ˆå™¨ [ID: {request_id}]: æ”¶åˆ°æ•°æ®: {data}")

            # Update request status to processing when we receive data
            if request_id:
                request_manager.update_status(request_id, RequestStatus.PROCESSING)

            # Handle both old and new request tracking systems
            if request_id in response_channels:
                queue = response_channels[request_id]
                logging.debug(f"æµè§ˆå™¨ [ID: {request_id}]: æ”¾å…¥é˜Ÿåˆ—å‰å¤§å°: {queue.qsize()}")
                await queue.put(data)
                logging.debug(f"æµè§ˆå™¨ [ID: {request_id}]: æ•°æ®å·²æ”¾å…¥é˜Ÿåˆ—ã€‚æ–°å¤§å°: {queue.qsize()}")

                # Check if this is the end of the request
                if data == "[DONE]":
                    request_manager.complete_request(request_id)

            else:
                # Check if this is a persistent request
                persistent_req = request_manager.get_request(request_id)
                if persistent_req:
                    logging.info(f"ğŸ”„ æ­£åœ¨æ¢å¤æŒä¹…è¯·æ±‚çš„é˜Ÿåˆ—: {request_id}")
                    response_channels[request_id] = persistent_req.response_queue
                    await persistent_req.response_queue.put(data)
                    request_manager.update_status(request_id, RequestStatus.PROCESSING)

                    if data == "[DONE]":
                        request_manager.complete_request(request_id)
                else:
                    logging.warning(f"âš ï¸ æµè§ˆå™¨: æ”¶åˆ°æœªçŸ¥/å·²å…³é—­çš„è¯·æ±‚æ¶ˆæ¯: {request_id}")

    except WebSocketDisconnect:
        logging.warning("âŒ æµè§ˆå™¨å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥")
    finally:
        browser_ws = None
        websocket_status.set(0)

        # Handle browser disconnect - keep persistent requests alive
        await request_manager.handle_browser_disconnect()

        # Only send errors to non-persistent requests
        for request_id, queue in response_channels.items():
            persistent_req = request_manager.get_request(request_id)
            if not persistent_req:  # Only error out non-persistent requests
                try:
                    await queue.put({"error": "Browser disconnected"})
                except KeyboardInterrupt:
                    raise
                except:
                    pass

        response_channels.clear()
        logging.info("WebSocket cleaned up. Persistent requests kept alive.")

# --- Monitor WebSocket ---
@app.websocket("/ws/monitor")
async def monitor_websocket(websocket: WebSocket):
    """ç›‘æ§é¢æ¿çš„WebSocketè¿æ¥"""
    await websocket.accept()
    monitor_clients.add(websocket)
    
    try:
        # å‘é€åˆå§‹æ•°æ®
        await websocket.send_json({
            "type": "initial_data",
            "active_requests": dict(realtime_stats.active_requests),
            "recent_requests": list(realtime_stats.recent_requests),
            "recent_errors": list(realtime_stats.recent_errors),
            "model_usage": dict(realtime_stats.model_usage)
        })
        
        while True:
            # ä¿æŒè¿æ¥
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        monitor_clients.remove(websocket)

# --- API Handler ---
@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    global request_manager

    if not browser_ws:
        raise HTTPException(status_code=503, detail="Browser client not connected.")

    openai_req = await request.json()
    request_id = str(uuid.uuid4())
    is_streaming = openai_req.get("stream", True)
    model_name = openai_req.get("model")

    model_info = MODEL_REGISTRY.get(model_name)
    if not model_info:
        raise HTTPException(status_code=404, detail=f"Model '{model_name}' not found.")
    model_type = model_info.get("type", "chat")

    # æ·»åŠ è¯·æ±‚å¼€å§‹æ—¥å¿—
    request_params = {
        "temperature": openai_req.get("temperature"),
        "top_p": openai_req.get("top_p"),
        "max_tokens": openai_req.get("max_tokens"),
        "streaming": is_streaming
    }
    messages = openai_req.get("messages", [])
    log_request_start(request_id, model_name, request_params, messages)
    
    # å¹¿æ’­åˆ°ç›‘æ§å®¢æˆ·ç«¯
    await broadcast_to_monitors({
        "type": "request_start",
        "request_id": request_id,
        "model": model_name,
        "timestamp": time.time()
    })

    # Create response queue and add to both systems for compatibility
    response_queue = asyncio.Queue(maxsize=Config.BACKPRESSURE_QUEUE_SIZE)
    response_channels[request_id] = response_queue

    # Add to persistent request manager
    try:
        persistent_req = await request_manager.add_request(
            request_id=request_id,
            openai_request=openai_req,
            response_queue=response_queue,
            model_name=model_name,
            is_streaming=is_streaming
        )
    except HTTPException:
        # å¹¶å‘é™åˆ¶
        log_request_end(request_id, False, 0, 0, "Too many concurrent requests")
        raise

    logging.info(f"API [ID: {request_id}]: Created persistent request for model type '{model_type}'.")

    try:
        task = asyncio.create_task(send_to_browser_task(request_id, openai_req))
        background_tasks.add(task)
        task.add_done_callback(background_tasks.discard)

        media_type = "text/event-stream" if is_streaming else "application/json"
        headers = {
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Transfer-Encoding": "chunked"
        } if is_streaming else {}

        logging.info(f"API [ID: {request_id}]: Returning {media_type} response to client.")

        if is_streaming:
            # Use custom streaming response for immediate flush
            return ImmediateStreamingResponse(
                stream_generator(request_id, model_name, is_streaming=is_streaming, model_type=model_type),
                media_type=media_type,
                headers=headers
            )
        else:
            # Use regular response for non-streaming
            return StreamingResponse(
                stream_generator(request_id, model_name, is_streaming=is_streaming, model_type=model_type),
                media_type=media_type,
                headers=headers
            )
    except KeyboardInterrupt:
        # Clean up on keyboard interrupt
        if request_id in response_channels:
            del response_channels[request_id]
        request_manager.complete_request(request_id)
        raise
    except Exception as e:
        # Clean up both tracking systems
        if request_id in response_channels:
            del response_channels[request_id]
        request_manager.complete_request(request_id)
        logging.error(f"API [ID: {request_id}]: Exception: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


async def send_to_browser_task(request_id: str, openai_req: dict):
    """This task runs in the background, sending the request to the browser."""
    global request_manager

    if not browser_ws:
        logging.error(f"TASK [ID: {request_id}]: Cannot send, browser disconnected.")
        # Mark request as error if browser is not connected
        persistent_req = request_manager.get_request(request_id)
        if persistent_req:
            await persistent_req.response_queue.put({"error": "Browser not connected"})
        return

    try:
        lmarena_payload, files_to_upload = create_lmarena_request_body(openai_req)

        message_to_browser = {
            "request_id": request_id,
            "payload": lmarena_payload,
            "files_to_upload": files_to_upload
        }

        logging.info(f"TASK [ID: {request_id}]: Sending payload and {len(files_to_upload)} file(s) to browser.")
        await browser_ws.send_text(json.dumps(message_to_browser))

        # Mark as sent to browser in persistent request manager
        request_manager.mark_sent_to_browser(request_id)
        logging.info(f"TASK [ID: {request_id}]: Payload sent and marked as sent to browser.")

    except KeyboardInterrupt:
        raise
    except Exception as e:
        logging.error(f"Error creating or sending request body: {e}", exc_info=True)

        # Send error to both tracking systems
        if request_id in response_channels:
            await response_channels[request_id].put({"error": f"Failed to process request: {e}"})

        persistent_req = request_manager.get_request(request_id)
        if persistent_req:
            await persistent_req.response_queue.put({"error": f"Failed to process request: {e}"})
            request_manager.update_status(request_id, RequestStatus.ERROR)


# Simple token estimation function
def estimateTokens(text: str) -> int:
    """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
    if not text:
        return 0
    # ç²—ç•¥ä¼°ç®—ï¼šå¹³å‡æ¯ä¸ªtokençº¦4ä¸ªå­—ç¬¦
    return len(str(text)) // 4


# --- Stream Consumer ---
async def stream_generator(request_id: str, model: str, is_streaming: bool, model_type: str):
    global request_manager, browser_ws
    start_time = time.time()  # æ·»åŠ å¼€å§‹æ—¶é—´è®°å½•

    # Get queue from either response_channels or persistent request
    queue = response_channels.get(request_id)
    persistent_req = request_manager.get_request(request_id)

    if not queue and persistent_req:
        queue = persistent_req.response_queue
        # Restore to response_channels for compatibility
        response_channels[request_id] = queue

    if not queue:
        logging.error(f"STREAMER [ID: {request_id}]: Queue not found!")
        return

    logging.info(f"STREAMER [ID: {request_id}]: Generator started for model type '{model_type}'.")
    await asyncio.sleep(0)

    response_id = f"chatcmpl-{uuid.uuid4()}"

    try:
        accumulated_content = ""
        media_urls = []
        finish_reason = None

        # Buffer for streaming chunks (minimum 50 chars)
        streaming_buffer = ""
        MIN_CHUNK_SIZE = 40
        last_chunk_time = time.time()
        MAX_BUFFER_TIME = 0.5  # Max 500ms before forcing flush

        while True:
            # Try to get data with a timeout to check buffer periodically
            try:
                raw_data = await asyncio.wait_for(queue.get(), timeout=0.1)
            except asyncio.TimeoutError:
                # No new data, but check if we should flush buffer
                if is_streaming and model_type == "chat" and streaming_buffer:
                    current_time = time.time()
                    if current_time - last_chunk_time >= MAX_BUFFER_TIME:
                        chunk = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {
                                    "role": "assistant",
                                    "content": streaming_buffer
                                },
                                "finish_reason": None
                            }],
                            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
                        }
                        chunk_data = f"data: {json.dumps(chunk)}\n\n"
                        yield chunk_data
                        streaming_buffer = ""
                        last_chunk_time = current_time
                continue

            if raw_data == "[DONE]":
                break

            # Handle error dictionary from timeout or browser disconnect
            if isinstance(raw_data, dict) and "error" in raw_data:
                logging.error(f"STREAMER [ID: {request_id}]: Received error: {raw_data}")

                # Format error for OpenAI response
                openai_error = {
                    "error": {
                        "message": str(raw_data.get("error", "Unknown error")),
                        "type": "server_error",
                        "code": None
                    }
                }

                if is_streaming:
                    yield f"data: {json.dumps(openai_error)}\n\ndata: [DONE]\n\n"
                else:
                    yield json.dumps(openai_error)
                return

            # First, try to detect if this is a JSON error response from the server
            if isinstance(raw_data, str) and raw_data.strip().startswith('{'):
                try:
                    error_data = json.loads(raw_data.strip())
                    if "error" in error_data:
                        logging.error(f"STREAMER [ID: {request_id}]: Server returned error: {error_data}")

                        # Parse the actual error structure from the server
                        server_error = error_data["error"]

                        # If the server error is already in OpenAI format, use it directly
                        if isinstance(server_error, dict) and "message" in server_error:
                            openai_error = {"error": server_error}
                        else:
                            # If it's just a string, wrap it in OpenAI format
                            openai_error = {
                                "error": {
                                    "message": str(server_error),
                                    "type": "server_error",
                                    "code": None
                                }
                            }

                        if is_streaming:
                            yield f"data: {json.dumps(openai_error)}\n\ndata: [DONE]\n\n"
                        else:
                            yield json.dumps(openai_error)
                        return
                except json.JSONDecodeError:
                    pass  # Not a JSON error, continue with normal parsing

            # Skip processing if raw_data is not a string (e.g., error dict)
            if not isinstance(raw_data, str):
                logging.warning(f"STREAMER [ID: {request_id}]: Skipping non-string data: {type(raw_data)}")
                continue

            try:
                prefix, content = raw_data.split(":", 1)

                if model_type in ["image", "video"] and prefix == "a2":
                    media_data_list = json.loads(content)
                    for item in media_data_list:
                        url = item.get("image") if model_type == "image" else item.get("url")
                        if url:
                            logging.info(f"MEDIA [ID: {request_id}]: Found {model_type} URL: {url}")
                            media_urls.append(url)

                elif model_type == "chat" and prefix == "a0":
                    delta = json.loads(content)
                    if is_streaming:
                        # Add to buffer instead of sending immediately
                        streaming_buffer += delta

                        # Check if we should send: either buffer is full or timeout reached
                        current_time = time.time()
                        time_since_last = current_time - last_chunk_time

                        if len(streaming_buffer) >= MIN_CHUNK_SIZE or (
                                streaming_buffer and time_since_last >= MAX_BUFFER_TIME):
                            chunk = {
                                "id": response_id,
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": model,
                                "choices": [{
                                    "index": 0,
                                    "delta": {
                                        "role": "assistant",
                                        "content": streaming_buffer
                                    },
                                    "finish_reason": None
                                }],
                                "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
                            }
                            chunk_data = f"data: {json.dumps(chunk)}\n\n"
                            yield chunk_data
                            
                            # ç´¯ç§¯å†…å®¹ç”¨äºè¯·æ±‚è¯¦æƒ…
                            accumulated_content += streaming_buffer
                            
                            # Clear buffer and update time after sending
                            streaming_buffer = ""
                            last_chunk_time = current_time
                    else:
                        accumulated_content += delta

                elif prefix == "ad":
                    finish_data = json.loads(content)
                    finish_reason = finish_data.get("finishReason", "stop")

            except (ValueError, json.JSONDecodeError):
                logging.warning(f"STREAMER [ID: {request_id}]: Could not parse data: {raw_data}")
                continue

            # Yield control to event loop after processing
            if is_streaming and model_type == "chat":
                await asyncio.sleep(0.001)  # Small delay to help with network flush
            else:
                await asyncio.sleep(0)

        # --- Final Response Generation ---

        # Flush any remaining buffer content for streaming chat
        if is_streaming and model_type == "chat" and streaming_buffer:
            chunk = {
                "id": response_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "role": "assistant",
                        "content": streaming_buffer
                    },
                    "finish_reason": None
                }],
                "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
            }
            yield f"data: {json.dumps(chunk)}\n\n"
            accumulated_content += streaming_buffer
            streaming_buffer = ""

        if model_type in ["image", "video"]:
            logging.info(f"MEDIA [ID: {request_id}]: Found {len(media_urls)} media file(s). Returning URLs directly.")
            # Format the URLs based on their type
            if model_type == "video":
                accumulated_content = "\n".join(media_urls)  # Return raw URLs for videos
            else:  # Default to image handling
                accumulated_content = "\n".join([f"![Generated Image]({url})" for url in media_urls])

        if is_streaming:
            if model_type in ["image", "video"]:
                chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "role": "assistant",
                            "content": accumulated_content
                        },
                        "finish_reason": finish_reason or "stop"
                    }],
                    "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
                }
                yield f"data: {json.dumps(chunk)}\n\n"

            # Send final chunk with finish_reason for chat models
            if model_type == "chat":
                final_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": finish_reason or "stop"
                    }],
                    "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"

            # Send [DONE] immediately
            yield "data: [DONE]\n\n"
        else:
            # For non-streaming, send the complete JSON object with the URL content
            complete_response = {
                "id": response_id,
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": accumulated_content
                    },
                    "finish_reason": finish_reason or "stop"
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                },
                "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
            }
            yield json.dumps(complete_response)

        # è®°å½•è¯·æ±‚æˆåŠŸ
        input_tokens = estimateTokens(str(persistent_req.openai_request if persistent_req else {}))
        output_tokens = estimateTokens(accumulated_content)
        log_request_end(request_id, True, input_tokens, output_tokens, response_content=accumulated_content)
        
        # å¹¿æ’­åˆ°ç›‘æ§å®¢æˆ·ç«¯
        await broadcast_to_monitors({
            "type": "request_end",
            "request_id": request_id,
            "success": True,
            "duration": time.time() - start_time
        })

    except asyncio.CancelledError:
        logging.warning(f"GENERATOR [ID: {request_id}]: Client disconnected.")

        # Send abort message to browser if WebSocket is connected
        if browser_ws:
            try:
                await browser_ws.send_text(json.dumps({
                    "type": "abort_request",
                    "request_id": request_id
                }))
                logging.info(f"GENERATOR [ID: {request_id}]: Sent abort message to browser")
            except Exception as e:
                logging.error(f"GENERATOR [ID: {request_id}]: Failed to send abort message: {e}")

        # Re-raise to properly handle the cancellation
        raise

    except KeyboardInterrupt:
        logging.info(f"GENERATOR [ID: {request_id}]: Keyboard interrupt received, cleaning up...")
        raise
    except Exception as e:
        logging.error(f"GENERATOR [ID: {request_id}]: Error: {e}", exc_info=True)
        
        # è®°å½•é”™è¯¯
        log_error(request_id, type(e).__name__, str(e), traceback.format_exc())
        log_request_end(request_id, False, 0, 0, str(e))
        
        # å¹¿æ’­é”™è¯¯åˆ°ç›‘æ§å®¢æˆ·ç«¯
        await broadcast_to_monitors({
            "type": "request_error",
            "request_id": request_id,
            "error": str(e),
            "timestamp": time.time()
        })
        
    finally:
        # Clean up both tracking systems
        if request_id in response_channels:
            del response_channels[request_id]
            logging.info(f"GENERATOR [ID: {request_id}]: Cleaned up response channel.")

        # Mark request as completed in persistent manager
        request_manager.complete_request(request_id)


def create_lmarena_request_body(openai_req: dict) -> (dict, list):
    model_name = openai_req["model"]

    if model_name not in MODEL_REGISTRY:
        raise ValueError(f"Model '{model_name}' not found in registry. Available models: {list(MODEL_REGISTRY.keys())}")

    model_info = MODEL_REGISTRY[model_name]
    model_id = model_info.get("id", model_name)
    modality = model_info.get("type", "chat")
    evaluation_id = str(uuid.uuid4())

    files_to_upload = []
    processed_messages = []

    # Process messages to extract files and clean content
    for msg in openai_req['messages']:
        content = msg.get("content", "")
        new_msg = msg.copy()

        if isinstance(content, list):
            # Handle official multimodal content array
            text_parts = []
            for part in content:
                if part.get("type") == "text":
                    text_parts.append(part.get("text", ""))
                elif part.get("type") == "image_url":
                    image_url = part.get("image_url", {}).get("url", "")
                    match = re.match(r"data:(image/\w+);base64,(.*)", image_url)
                    if match:
                        mime_type, base64_data = match.groups()
                        file_ext = mime_type.split('/')
                        filename = f"upload-{uuid.uuid4()}.{file_ext}"
                        files_to_upload.append({"fileName": filename, "contentType": mime_type, "data": base64_data})
            new_msg["content"] = "\n".join(text_parts)
            processed_messages.append(new_msg)

        elif isinstance(content, str):
            # Handle simple string content that might contain data URLs
            text_content = content
            matches = re.findall(r"data:(image/\w+);base64,([a-zA-Z0-9+/=]+)", content)
            if matches:
                logging.info(f"Found {len(matches)} data URL(s) in string content.")
                for mime_type, base64_data in matches:
                    file_ext = mime_type.split('/')
                    filename = f"upload-{uuid.uuid4()}.{file_ext}"
                    files_to_upload.append({"fileName": filename, "contentType": mime_type, "data": base64_data})
                # Remove all found data URLs from the text to be sent
                text_content = re.sub(r"data:image/\w+;base64,[a-zA-Z0-9+/=]+", "", text_content).strip()

            new_msg["content"] = text_content
            processed_messages.append(new_msg)

        else:
            # If content is not a list or string, just pass it through
            processed_messages.append(msg)

    # Find the last user message index
    last_user_message_index = -1
    for i in range(len(processed_messages) - 1, -1, -1):
        if processed_messages[i].get("role") == "user":
            last_user_message_index = i
            break

    # Insert empty user message after the last user message (only for chat models)
    if modality == "chat" and last_user_message_index != -1:
        # Insert empty user message after the last user message
        insert_index = last_user_message_index + 1
        empty_user_message = {"role": "user", "content": " "}
        processed_messages.insert(insert_index, empty_user_message)
        logging.info(
            f"Added empty user message after last user message at index {last_user_message_index} for chat model")

    # Build Arena-formatted messages
    arena_messages = []
    message_ids = [str(uuid.uuid4()) for _ in processed_messages]
    for i, msg in enumerate(processed_messages):
        parent_message_ids = [message_ids[i - 1]] if i > 0 else []

        original_role = msg.get("role")
        role = "user" if original_role not in ["user", "assistant", "data"] else original_role

        arena_messages.append({
            "id": message_ids[i], "role": role, "content": msg['content'],
            "experimental_attachments": [], "parentMessageIds": parent_message_ids,
            "participantPosition": "a", "modelId": model_id if role == 'assistant' else None,
            "evaluationSessionId": evaluation_id, "status": "pending", "failureReason": None,
        })

    user_message_id = message_ids[-1] if message_ids else str(uuid.uuid4())
    model_a_message_id = str(uuid.uuid4())
    arena_messages.append({
        "id": model_a_message_id, "role": "assistant", "content": "",
        "experimental_attachments": [], "parentMessageIds": [user_message_id],
        "participantPosition": "a", "modelId": model_id,
        "evaluationSessionId": evaluation_id, "status": "pending", "failureReason": None,
    })

    payload = {
        "id": evaluation_id, "mode": "direct", "modelAId": model_id,
        "userMessageId": user_message_id, "modelAMessageId": model_a_message_id,
        "messages": arena_messages, "modality": modality,
    }

    return payload, files_to_upload


@app.get("/v1/models")
async def get_models():
    """Lists all available models in an OpenAI-compatible format."""
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "created": int(asyncio.get_event_loop().time()),
                "owned_by": "lmarena",
                "type": model_info.get("type", "chat")
            }
            for model_name, model_info in MODEL_REGISTRY.items()
        ],
    }


@app.post("/v1/refresh-models")
async def refresh_models():
    """Request model registry refresh from browser script."""
    if browser_ws:
        try:
            # Send refresh request to browser
            await browser_ws.send_text(json.dumps({
                "type": "refresh_models"
            }))

            return {
                "success": True,
                "message": "Model refresh request sent to browser",
                "models": list(MODEL_REGISTRY.keys())
            }
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.error(f"Failed to send refresh request: {e}")
            return {
                "success": False,
                "message": "Failed to send refresh request to browser",
                "models": list(MODEL_REGISTRY.keys())
            }
    else:
        return {
            "success": False,
            "message": "No browser connection available",
            "models": list(MODEL_REGISTRY.keys())
        }

# --- Prometheus Metrics Endpoint ---
@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

# --- ç›‘æ§ç›¸å…³APIç«¯ç‚¹ ---
@app.get("/api/stats/summary")
async def get_stats_summary():
    """è·å–ç»Ÿè®¡æ‘˜è¦"""
    # ä»æ—¥å¿—æ–‡ä»¶è®¡ç®—ç»Ÿè®¡
    recent_logs = log_manager.read_request_logs(limit=10000)  # è¯»å–æœ€è¿‘çš„æ—¥å¿—
    
    # è®¡ç®—24å°æ—¶å†…çš„ç»Ÿè®¡
    current_time = time.time()
    day_ago = current_time - 86400
    
    recent_24h_logs = [log for log in recent_logs if log.get('timestamp', 0) > day_ago]
    
    total_requests = len(recent_24h_logs)
    successful = sum(1 for log in recent_24h_logs if log.get('status') == 'success')
    failed = total_requests - successful
    
    total_input_tokens = sum(log.get('input_tokens', 0) for log in recent_24h_logs)
    total_output_tokens = sum(log.get('output_tokens', 0) for log in recent_24h_logs)
    
    durations = [log.get('duration', 0) for log in recent_24h_logs if log.get('duration', 0) > 0]
    avg_duration = sum(durations) / len(durations) if durations else 0
    
    # è·å–æ€§èƒ½ç»Ÿè®¡
    perf_stats = performance_monitor.get_stats()
    model_perf = performance_monitor.get_model_stats()
    
    # æ„å»ºæ¨¡å‹ç»Ÿè®¡
    model_stats = []
    for model_name, usage in realtime_stats.model_usage.items():
        perf = model_perf.get(model_name, {})
        model_stats.append({
            "model": model_name,
            "total_requests": usage['requests'],
            "successful_requests": usage['requests'] - usage['errors'],
            "failed_requests": usage['errors'],
            "total_input_tokens": usage.get('tokens', 0) // 2,  # ç²—ç•¥ä¼°ç®—
            "total_output_tokens": usage.get('tokens', 0) // 2,
            "avg_duration": perf.get('avg_response_time', 0),
            "qps": perf.get('qps', 0),
            "error_rate": perf.get('error_rate', 0)
        })
    
    return {
        "summary": {
            "total_requests": total_requests,
            "successful": successful,
            "failed": failed,
            "total_input_tokens": total_input_tokens,
            "total_output_tokens": total_output_tokens,
            "avg_duration": avg_duration,
            "success_rate": (successful / total_requests * 100) if total_requests > 0 else 0
        },
        "performance": perf_stats,
        "model_stats": sorted(model_stats, key=lambda x: x['total_requests'], reverse=True),
        "active_requests": len(realtime_stats.active_requests),
        "browser_connected": browser_ws is not None,
        "monitor_clients": len(monitor_clients),
        "uptime": time.time() - startup_time
    }

@app.get("/api/logs/requests")
async def get_request_logs(limit: int = 100, offset: int = 0, model: str = None):
    """è·å–è¯·æ±‚æ—¥å¿—"""
    logs = log_manager.read_request_logs(limit, offset, model)
    return logs

@app.get("/api/logs/errors")
async def get_error_logs(limit: int = 50):
    """è·å–é”™è¯¯æ—¥å¿—"""
    logs = log_manager.read_error_logs(limit)
    return logs

@app.get("/api/logs/download")
async def download_logs(log_type: str = "requests"):
    """ä¸‹è½½æ—¥å¿—æ–‡ä»¶"""
    if log_type == "requests":
        file_path = log_manager.request_log_path
        filename = "requests.jsonl"
    elif log_type == "errors":
        file_path = log_manager.error_log_path
        filename = "errors.jsonl"
    else:
        raise HTTPException(status_code=400, detail="Invalid log type")
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Log file not found")
    
    return StreamingResponse(
        open(file_path, 'rb'),
        media_type="application/x-jsonlines",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )

@app.get("/api/request/{request_id}")
async def get_request_details(request_id: str):
    """è·å–è¯·æ±‚è¯¦æƒ…"""
    details = request_details_storage.get(request_id)
    if not details:
        raise HTTPException(status_code=404, detail="Request details not found")
    
    return {
        "request_id": details.request_id,
        "timestamp": details.timestamp,
        "model": details.model,
        "status": details.status,
        "duration": details.duration,
        "input_tokens": details.input_tokens,
        "output_tokens": details.output_tokens,
        "error": details.error,
        "request_params": details.request_params,
        "request_messages": details.request_messages,
        "response_content": details.response_content,
        "headers": details.headers
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "browser_connected": browser_ws is not None,
        "active_requests": len(request_manager.active_requests),
        "uptime": time.time() - startup_time,
        "models_loaded": len(MODEL_REGISTRY),
        "monitor_clients": len(monitor_clients),
        "log_files": {
            "requests": str(log_manager.request_log_path),
            "errors": str(log_manager.error_log_path)
        }
    }



# --- é…ç½®ç®¡ç†API ---
@app.get("/api/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    return config_manager.dynamic_config


@app.post("/api/config")
async def update_config(request: Request):
    """æ›´æ–°é…ç½®"""
    try:
        config_data = await request.json()

        # æ›´æ–°é…ç½®
        config_manager._deep_merge(config_manager.dynamic_config, config_data)
        config_manager.save_config()

        # åº”ç”¨æŸäº›é…ç½®çš„å³æ—¶æ›´æ”¹
        if 'request' in config_data:
            if 'timeout_seconds' in config_data['request']:
                Config.REQUEST_TIMEOUT_SECONDS = config_data['request']['timeout_seconds']
            if 'max_concurrent_requests' in config_data['request']:
                Config.MAX_CONCURRENT_REQUESTS = config_data['request']['max_concurrent_requests']

        return {"status": "success", "message": "é…ç½®å·²æ›´æ–°"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/api/config/quick-links")
async def update_quick_links(request: Request):
    """æ›´æ–°å¿«é€Ÿé“¾æ¥"""
    try:
        links = await request.json()
        config_manager.set('quick_links', links)
        return {"status": "success", "message": "å¿«é€Ÿé“¾æ¥å·²æ›´æ–°"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.get("/api/system/info")
async def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    display_ip = config_manager.get_display_ip()
    port = config_manager.get('network.port', Config.PORT)

    return {
        "server_urls": {
            "local": f"http://localhost:{port}",
            "network": f"http://{display_ip}:{port}",
            "monitor": f"http://{display_ip}:{port}/monitor",
            "metrics": f"http://{display_ip}:{port}/metrics",
            "health": f"http://{display_ip}:{port}/api/health/detailed"
        },
        "detected_ips": get_all_local_ips(),
        "current_ip": display_ip,
        "auto_detect": config_manager.get('network.auto_detect_ip', True)
    }


def get_all_local_ips():
    """è·å–æ‰€æœ‰æœ¬åœ°IPåœ°å€"""
    import socket
    ips = []
    try:
        hostname = socket.gethostname()
        all_ips = socket.gethostbyname_ex(hostname)[2]
        for ip in all_ips:
            if not ip.startswith('127.') and not ip.startswith('198.18.'):
                ips.append(ip)
    except:
        pass
    return ips


@app.get("/api/health/detailed")
async def get_detailed_health():
    """è·å–ç®€åŒ–çš„å¥åº·çŠ¶æ€"""
    # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
    active_count = len(realtime_stats.active_requests)
    uptime_seconds = time.time() - startup_time
    
    # ç®€å•çš„æ€§èƒ½ç»Ÿè®¡
    perf_stats = performance_monitor.get_stats() if 'performance_monitor' in globals() else {}
    
    return {
        "status": "healthy" if browser_ws else "disconnected",
        "browser_connected": browser_ws is not None,
        "active_requests": active_count,
        "max_concurrent_requests": Config.MAX_CONCURRENT_REQUESTS,
        "capacity_usage_percent": (active_count / Config.MAX_CONCURRENT_REQUESTS * 100),
        "models_loaded": len(MODEL_REGISTRY),
        "monitor_clients": len(monitor_clients),
        "uptime": {
            "seconds": uptime_seconds,
            "hours": uptime_seconds / 3600,
            "days": uptime_seconds / 86400
        },
        "performance": {
            "avg_response_time": perf_stats.get('avg_response_time', 0)
        },
        "log_files": {
            "requests": str(log_manager.request_log_path),
            "errors": str(log_manager.error_log_path)
        }
    }


# --- ç›‘æ§é¢æ¿HTML (ä¼˜åŒ–ç‰ˆ) ---
@app.get("/monitor", response_class=HTMLResponse)
async def monitor_dashboard():
    """è¿”å›ç›‘æ§é¢æ¿HTMLé¡µé¢"""
    # è¯»å–å¤–éƒ¨HTMLæ–‡ä»¶
    html_file_path = Path(__file__).parent / "index.html"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not html_file_path.exists():
        return HTMLResponse(
            content="<h1>ç›‘æ§é¢æ¿æ–‡ä»¶æœªæ‰¾åˆ°</h1><p>è¯·ç¡®ä¿ index.html æ–‡ä»¶åœ¨æ­£ç¡®çš„ä½ç½®ã€‚</p>",
            status_code=404
        )
    
    # è¯»å–å¹¶è¿”å›HTMLå†…å®¹
    with open(html_file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    return HTMLResponse(content=html_content)

print("\n" + "="*60)
print("ğŸš€ LMArena åå‘ä»£ç†æœåŠ¡å™¨")
print("="*60)
print(f"ğŸ“ æœ¬åœ°è®¿é—®: http://localhost:{Config.PORT}")
print(f"ğŸ“ å±€åŸŸç½‘è®¿é—®: http://{get_local_ip()}:{Config.PORT}")
print(f"ğŸ“Š ç›‘æ§é¢æ¿: http://{get_local_ip()}:{Config.PORT}/monitor")
print("="*60)
print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿æµè§ˆå™¨æ‰©å±•å·²å®‰è£…å¹¶å¯ç”¨")
print("ğŸ’¡ å¦‚æœä½¿ç”¨ä»£ç†è½¯ä»¶ï¼Œå±€åŸŸç½‘IPå¯èƒ½ä¸å‡†ç¡®")
print("ğŸ’¡ å¦‚æœå±€åŸŸç½‘IPä¸å‡†ç¡®å¯ä»¥åœ¨æ­¤æ–‡ä»¶ä¸­ä¿®æ”¹ï¼Œå°†MANUAL_IP = None  ä¿®æ”¹ä¸ºMANUAL_IP = ä½ æŒ‡å®šçš„IPåœ°å€å¦‚ï¼š192.168.0.1")
print("="*60 + "\n")
if __name__ == "__main__":
    uvicorn.run(app, host=Config.HOST, port=Config.PORT)
